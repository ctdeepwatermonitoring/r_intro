y <- x + rnorm(100)
plot(x = x, y = y, col =  my.cols,
pch = 16,
cex = runif(100, min = 0, max = 2),
main = "piratepal('google', trans = .5)")
set.seed(100) # For reproducibility
x <- rnorm(100)
y <- x + rnorm(100)
plot(x = x, y = y, col =  my.cols,
pch = 16,
cex = runif(100, min = 0, max = 2),
main = "piratepal('google', trans = .5)")
set.seed(100) # For reproducibility
x <- rnorm(100)
y <- x + rnorm(100)
plot(x = x, y = y, col =  my.cols,
pch = 16,
cex = runif(100, min = 0, max = 2),
main = "piratepal('google', trans = .5)")
set.seed(100) # For reproducibility
x <- rnorm(100)
y <- x + rnorm(100)
plot(x = x, y = y, col =  my.cols,
pch = 16,
cex = runif(100, min = 0, max = 2),
main = "piratepal('pony', trans = .5)")
set.seed(100) # For reproducibility
x <- rnorm(100)
y <- x + rnorm(100)
plot(x = x, y = y, col =  my.cols,
pch = 16,
cex = runif(100, min = 0, max = 2),
main = "piratepal('pony', trans = .5)")
set.seed(100) # For reproducibility
x <- rnorm(100)
y <- x + rnorm(100)
plot(x = x, y = y, col =  my.cols,
pch = 17,
cex = runif(100, min = 0, max = 2),
main = "piratepal('google', trans = .5)")
my.cols <- piratepal(palette = "pony",
trans = .5)
set.seed(200) # For reproducibility
x <- rnorm(200)
y <- x + rnorm(200)
plot(x = x, y = y, col =  my.cols,
pch = 16,
cex = runif(100, min = 0, max = 2),
main = "piratepal('pony', trans = .5)")
my.cols <- piratepal(palette = "pony",
trans = .5)
set.seed(200) # For reproducibility
x <- rnorm(200)
y <- x + rnorm(200)
plot(x = x, y = y, col =  my.cols,
pch = 16,
cex = runif(100, min = 0, max = 2),
main = "piratepal('pony', trans = .5)")
my.cols <- piratepal(palette = "pony",
trans = .5)
set.seed(100) # For reproducibility
x <- rnorm(100)
y <- x + rnorm(100)
plot(x = x, y = y, col =  my.cols,
pch = 16,
cex = runif(100, min = 0, max = 2),
main = "piratepal('pony', trans = .5)")
my.cols <- piratepal(palette = "pony",
trans = .5)
set.seed(100) # For reproducibility
x <- rnorm(100)
y <- x + rnorm(100)
plot(x = x, y = y, col =  my.cols,
pch = 16,
cex = runif(100, min = 0, max = 2),
main = "piratepal('pony', trans = .5)")
setwd("P:/R/IR")
library(dplyr)
library(leaflet)
library(sf)
library(sp)
library(mapview)
#for the queries
results_list <- list() #empty list to populate
stations_list <- list() #empty list to populate
orgs <- c("USGS-CT", "CTVOLMON", "FRWA", "TLGVWQMPROGRAM") #add orgs as we identify them
#can change elements of this URL
base_URL <- "https://www.waterqualitydata.us/data/requesttype/search?statecode=US%3A09&organization=orgs&characteristicName=Escherichia%20coli&startDateLo=startdate&startDateHi=enddate&mimeType=csv&zip=no&providers=NWIS&providers=STEWARDS&providers=STORET"
#date portion
startdate <- "01-01-2021" #change if needed
enddate <- "12-31-2023" #change if needed
base_URL <- gsub("startdate", startdate, base_URL)
base_URL <- gsub("enddate", enddate, base_URL)
######getting sample results######
#type of table
result_URL <- base_URL
result_URL <- gsub("requesttype", "Result", result_URL)
for (i in orgs) {
request_result_URL <- gsub("orgs", i, result_URL)
data <- read.csv(request_result_URL)
results_list[[i]] <- data
}
View(results_list)
setwd("P:/R/IR") #change as needed
#needed packages
library(dplyr)
library(leaflet)
library(sf)
library(sp)
library(mapview)
################################################################################
#####querying the WQP directly for E coli stations##############################
results_list <- list() #empty list to populate with E coli results
stations_list <- list() #empty list to populate with E coli stations
orgs <- c("USGS-CT", "CTVOLMON", "FRWA", "TLGVWQMPROGRAM") #add orgs as we identify them
#can change elements of this URL
base_URL <- "https://www.waterqualitydata.us/data/requesttype/search?statecode=US%3A09&organization=orgs&characteristicName=Escherichia%20coli&startDateLo=startdate&startDateHi=enddate&mimeType=csv&zip=no&providers=NWIS&providers=STEWARDS&providers=STORET"
#date portion
startdate <- "01-01-2022" #change if needed
enddate <- "12-31-2023" #change if needed
base_URL <- gsub("startdate", startdate, base_URL)
base_URL <- gsub("enddate", enddate, base_URL)
#type of table we query from WQP
station_URL <- base_URL
station_URL <- gsub("requesttype", "Station", base_URL)
#looping over orgs list into station URl request, each iteration is stored in list
for (i in orgs) {
request_station_URL <- gsub("orgs", i, station_URL)
stations <- read.csv(request_station_URL)
stations_list[[i]] <- stations
}
merged_stations <- do.call(rbind, stations_list) #merging each loop iteration
write.csv(merged_stations, "merged_stations.csv", row.names = FALSE) #save to reference if needed
################################################################################
#######code for snapping WQX sites to DEEP segments#############################
#shapefiles for snapped map - QA use to check how points changed
rivers <- read_sf(dsn = ".", layer = "CT_305b_Assessed_River_2022")
basins <- read_sf(dsn = ".", layer = "Drainage_Basin_Polygon")
basins <- st_make_valid(basins) #Loop 0 is not valid: Edge 225 has duplicate vertex with edge 267?
sites_sf <- st_as_sf(merged_stations, coords=c("LongitudeMeasure","LatitudeMeasure"), crs =4326)
#function source: https://mapping-in-r-workshop.ryanpeek.org/03_vig_snapping_points_to_line
st_snap_points <- function(x, y, namevar, max_dist = 1000) {
# this evaluates the length of the data
if (inherits(x, "sf")) n = nrow(x)
if (inherits(x, "sfc")) n = length(x)
# this part:
# 1. loops through every piece of data (every point)
# 2. snaps a point to the nearest line geometries
# 3. calculates the distance from point to line geometries
# 4. retains only the shortest distances and generates a point at that intersection
out = do.call(c,
lapply(seq(n), function(i) {
nrst = st_nearest_points(st_geometry(x)[i], y)
nrst_len = st_length(nrst)
nrst_mn = which.min(nrst_len)
if (as.vector(nrst_len[nrst_mn]) > max_dist) return(st_geometry(x)[i])
return(st_cast(nrst[nrst_mn], "POINT")[2])
})
)
# this part converts the data to a dataframe and adds a named column of your choice
out_xy <- st_coordinates(out) %>% as.data.frame()
out_xy <- out_xy %>%
mutate({{namevar}} := x[[namevar]]) %>%
st_as_sf(coords=c("X","Y"), crs=st_crs(x), remove=FALSE)
return(out_xy)
}
#snapping and joining
sites_sf <- st_transform(sites_sf, crs = 4326) #making same coordinate system
rivers <- st_transform(rivers, crs = 4326) #making same coordinate system
sites_snapped <- st_snap_points(sites_sf, rivers, namevar = "MonitoringLocationIdentifier", max_dist = 30) #function from above
setwd("P:/R/IR") #change as needed
#needed packages
library(dplyr)
library(leaflet)
library(sf)
library(sp)
library(mapview)
################################################################################
#####querying the WQP directly for E coli stations##############################
results_list <- list() #empty list to populate with E coli results
stations_list <- list() #empty list to populate with E coli stations
orgs <- c("USGS-CT", "CTVOLMON", "FRWA", "TLGVWQMPROGRAM") #add orgs as we identify them
#can change elements of this URL
base_URL <- "https://www.waterqualitydata.us/data/requesttype/search?statecode=US%3A09&organization=orgs&characteristicName=Escherichia%20coli&startDateLo=startdate&startDateHi=enddate&mimeType=csv&zip=no&providers=NWIS&providers=STEWARDS&providers=STORET"
#date portion
startdate <- "01-01-2022" #change if needed
enddate <- "12-31-2023" #change if needed
base_URL <- gsub("startdate", startdate, base_URL)
base_URL <- gsub("enddate", enddate, base_URL)
#type of table we query from WQP
station_URL <- base_URL
station_URL <- gsub("requesttype", "Station", base_URL)
#looping over orgs list into station URl request, each iteration is stored in list
for (i in orgs) {
request_station_URL <- gsub("orgs", i, station_URL)
stations <- read.csv(request_station_URL)
stations_list[[i]] <- stations
}
merged_stations <- do.call(rbind, stations_list) #merging each loop iteration
write.csv(merged_stations, "merged_stations.csv", row.names = FALSE) #save to reference if needed
######bind E coli results to segmentQA file after adding and fixing segments####
#getting E coli results - duplicated code from above to readability.. may change
base_URL <- "https://www.waterqualitydata.us/data/requesttype/search?statecode=US%3A09&organization=orgs&characteristicName=Escherichia%20coli&startDateLo=startdate&startDateHi=enddate&mimeType=csv&zip=no&providers=NWIS&providers=STEWARDS&providers=STORET"
#date portion
startdate <- "01-01-2022" #change if needed
enddate <- "12-31-2023" #change if needed
base_URL <- gsub("startdate", startdate, base_URL)
base_URL <- gsub("enddate", enddate, base_URL)
result_URL <- base_URL
result_URL <- gsub("requesttype", "Result", result_URL)
for (i in orgs) {
request_result_URL <- gsub("orgs", i, result_URL)
data <- read.csv(request_result_URL)
results_list[[i]] <- data
}
merged_results <- do.call(rbind, results_list) #all E coli results with the request parameters
View(merged_stations)
View(merged_results)
merged_results$ActivityStartDate <- as.Date(merged_results$ActivityStartDate, format = "%Y-%m-d")
View(merged_results)
merged_results <- do.call(rbind, results_list) #all E coli results with the request parameters
merged_results$ActivityStartDate <- as.Date(merged_results$ActivityStartDate, format = "%y-%m-d")
View(merged_results)
merged_results <- do.call(rbind, results_list) #all E coli results with the request parameters
merged_results$ActivityStartDate <- as.Date(merged_results$ActivityStartDate, format = "%Y-%m-d")
View(merged_results)
merged_results <- do.call(rbind, results_list) #all E coli results with the request parameters
merged_results$ActivityStartDate <- as.POSIXct(merged_results$ActivityStartDate, format = "%Y-%m-d")
merged_results <- do.call(rbind, results_list) #all E coli results with the request parameters
merged_results$ActivityStartDate <- as.POSIXct(merged_results$ActivityStartDate)
merged_results <- subset(merged_results, merged_results$ActivityStartDate >= as.POSIXct("2022-05-01") &
merged_results$ActivityStartDate <= as.POSIXct("2022-10-01") &
merged_results$ActivityStartDate >= as.POSIXct("2023-05-01") &
merged_results$ActivityStartDate <= as.POSIXct("2022-10-01"))
View(merged_results)
merged_results <- do.call(rbind, results_list) #all E coli results with the request parameters
merged_results$ActivityStartDate <- as.POSIXct(merged_results$ActivityStartDate)
merged_results <- subset(merged_results, merged_results$ActivityStartDate >= as.POSIXct("2022-05-01"))
merged_results <- do.call(rbind, results_list) #all E coli results with the request parameters
merged_results$ActivityStartDate <- as.POSIXct(merged_results$ActivityStartDate)
merged_results <- subset(merged_results, merged_results$ActivityStartDate >= as.POSIXct("2022-05-01") |
merged_results$ActivityStartDate <= as.POSIXct("2022-10-01") |
merged_results$ActivityStartDate >= as.POSIXct("2023-05-01") |
merged_results$ActivityStartDate <= as.POSIXct("2022-10-01"))
View(merged_results)
merged_results <- do.call(rbind, results_list) #all E coli results with the request parameters
merged_results$ActivityStartDate <- as.POSIXct(merged_results$ActivityStartDate)
merged_results <- subset(merged_results, merged_results$ActivityStartDate >= as.POSIXct("2022-05-01") &
merged_results$ActivityStartDate <= as.POSIXct("2022-10-01") |
merged_results$ActivityStartDate >= as.POSIXct("2023-05-01") &
merged_results$ActivityStartDate <= as.POSIXct("2022-10-01"))
View(merged_results)
merged_results <- do.call(rbind, results_list) #all E coli results with the request parameters
merged_results$ActivityStartDate <- as.POSIXct(merged_results$ActivityStartDate)
merged_results_2022 <- subset(merged_results, merged_results$ActivityStartDate >= as.POSIXct("2022-05-01") &
merged_results$ActivityStartDate <= as.POSIXct("2022-10-01"))
merged_results_2022 <- subset(merged_results, merged_results$ActivityStartDate >= as.POSIXct("2023-05-01") &
merged_results$ActivityStartDate <= as.POSIXct("2023-10-01"))
merged_results <- do.call(rbind, results_list) #all E coli results with the request parameters
merged_results$ActivityStartDate <- as.POSIXct(merged_results$ActivityStartDate)
merged_results_2022 <- subset(merged_results, merged_results$ActivityStartDate >= as.POSIXct("2022-05-01") &
merged_results$ActivityStartDate <= as.POSIXct("2022-10-01"))
merged_results_2023 <- subset(merged_results, merged_results$ActivityStartDate >= as.POSIXct("2023-05-01") &
merged_results$ActivityStartDate <= as.POSIXct("2023-10-01"))
View(merged_results_2022)
View(merged_results)
View(merged_results_2022)
View(merged_results_2023)
merged_results <- rbind(merged_results_2022, merged_results_2023) #couldnt do in one step due to filtering logic?
results_count <- merged_results %>%
group_by(MonitoringLocationIdentifier) %>%
summarize(ResultsPerSite = n())
View(results_count)
station_to_segment <- read.csv("station_to_segment_updated.csv") #this should be the UPDATED segment QA file
result_with_segment <- merged_results
result_with_segment <- merge(station_to_segment, result_with_segment, by = "MonitoringLocationIdentifier")
result_with_segment$ActivityConductingOrganizationText <- ifelse(is.na(result_with_segment$ActivityConductingOrganizationText),
result_with_segment$OrganizationIdentifier, result_with_segment$ActivityConductingOrganizationText) #create one column with org info
result_with_segment$ActivityConductingOrganizationText <- gsub("U.S. Geological Survey-Water Resources Discipline",
"U.S. Geological Survey", result_with_segment$ActivityConductingOrganizationText) #they call themselves two different things
result_with_segment <- result_with_segment[c("ASSESSMENT", "MonitoringLocationIdentifier", "ActivityConductingOrganizationText",
"ActivityStartDate", "CharacteristicName", "ResultMeasureValue", "ResultMeasure.MeasureUnitCode",
"ResultDetectionConditionText")] #these are the columns we need for analysis
#creating file to manually check and fix things
write.csv(result_with_segment, "IR_Ecoli_with_segment.csv", row.names = FALSE)
result_with_segment <- read.csv("IR_Ecoli_with_segment.csv") #note: MANUALLY changed NA/null result values to either 1 or 2000 depending on detect limit comment
sapply(result_with_segment, class) #checking data types before doing maths
gmean <- function(x) exp(mean(log(x))) #define geomean function
#summarized table
segment_analysis <- result_with_segment %>%
group_by(ASSESSMENT) %>%
summarise(
nSamples = n(), #how many samples contributed to assessment
maxSample = max(ResultMeasureValue, na.rm = TRUE), #max sample in case there is something wild screwing the gmean
minSample = min(ResultMeasureValue, na.rm = TRUE), #informational
nOver410 = sum(ResultMeasureValue > 410), #how many samples over 410 MPN?
nOver576 = sum(ResultMeasureValue > 576), #how many samples over 576 MPN?
nOver1000 = sum(ResultMeasureValue > 1000), #how many samples over 1000 MPN?
meanSamples = mean(ResultMeasureValue), #regular mean
gmeanSamples = gmean(ResultMeasureValue), #from my handy geomean function
percentExceed = ((nOver576/nSamples) * 100), #this is how Walter calculated it
whichOrgs = toString(unique(ActivityConductingOrganizationText)), #which orgs contributed
nContributingOrgs = n_distinct(ActivityConductingOrganizationText) #how many orgs contributed (QA check on whichOrgs)
)
write.csv(segment_analysis, "segment_analysis.csv", row.names = FALSE) #to make final assessment decisions
View(segment_analysis)
assessed_2022 <- read.csv("305b_Assessed_2022.csv")
View(assessed_2022)
View(segment_analysis)
colnames(assessed_2022)[3] <- "ASSESSED"
View(assessed_2022)
colnames(assessed_2022)
assessed_2022 <- c("ASSESSED", "CT2022_REC_USE_ATTAINMENT")
assessed_2022 <- read.csv("305b_Assessed_2022.csv")
colnames(assessed_2022)[3] <- "ASSESSED"
assessed_2022 <- assessed_2022[c("ASSESSED", "CT2022_REC_USE_ATTAINMENT")]
segment_analysis <- left_join(segment_analysis, assessed_2022, by = "ASSESSED")
segment_analysis <- merge(segment_analysis, assessed_2022, by = "ASSESSED")
View(segment_analysis)
assessed_2022 <- read.csv("305b_Assessed_2022.csv")
colnames(assessed_2022)[3] <- "ASSESSMENT"
assessed_2022 <- assessed_2022[c("ASSESSMENT", "CT2022_REC_USE_ATTAINMENT")]
segment_analysis <- left_join(segment_analysis, assessed_2022, by = "ASSESSMENT")
write.csv(segment_analysis, "segment_analysis.csv", row.names = FALSE) #to make final assessment decisions
assessed_2022 <- read.csv("305b_Assessed_2022.csv")
colnames(assessed_2022)[3] <- "ASSESSMENT"
assessed_2022 <- assessed_2022[c("ASSESSMENT", "CT2022_REC_USE_ATTAINMENT")]
segment_analysis <- merge(segment_analysis, assessed_2022, by = "ASSESSMENT")
assessed_2022 <- read.csv("305b_Assessed_2022.csv")
colnames(assessed_2022)[3] <- "ASSESSMENT"
assessed_2022 <- assessed_2022[c("ASSESSMENT", "CT2022_REC_USE_ATTAINMENT")]
segment_analysis <- left_join(segment_analysis, assessed_2022, by = "ASSESSMENT")
segment_analysis <- result_with_segment %>%
group_by(ASSESSMENT) %>%
summarise(
nSamples = n(), #how many samples contributed to assessment
maxSample = max(ResultMeasureValue, na.rm = TRUE), #max sample in case there is something wild screwing the gmean
minSample = min(ResultMeasureValue, na.rm = TRUE), #informational
nOver410 = sum(ResultMeasureValue > 410), #how many samples over 410 MPN?
nOver576 = sum(ResultMeasureValue > 576), #how many samples over 576 MPN?
nOver1000 = sum(ResultMeasureValue > 1000), #how many samples over 1000 MPN?
meanSamples = mean(ResultMeasureValue), #regular mean
gmeanSamples = gmean(ResultMeasureValue), #from my handy geomean function
percentExceed = ((nOver576/nSamples) * 100), #this is how Walter calculated it
whichOrgs = toString(unique(ActivityConductingOrganizationText)), #which orgs contributed
nContributingOrgs = n_distinct(ActivityConductingOrganizationText) #how many orgs contributed (QA check on whichOrgs)
)
segment_analysis_2 <- right_join(segment_analysis, assessed_2022, by = "ASSESSMENT")
segment_analysis_2 <- left_join(segment_analysis, assessed_2022, by = "ASSESSMENT")
View(segment_analysis_2)
View(segment_analysis_2)
riverassessed_2022 <- read.csv("305b_Assessed_2022_River.csv")
lakeassessed_2022 <- read.csv("305b_Assessed_2022_Lake.csv")
assessed_2022 <- rbind(riverassessed_2022, lakeassessed_2022)
View(lakeassessed_2022)
View(lakeassessed_2022)
riverassessed_2022 <- read.csv("305b_Assessed_2022_River.csv")
lakeassessed_2022 <- read.csv("305b_Assessed_2022_Lake.csv")
colnames(riverassessed_2022)[3] <- "ASSESSMENT"
colnames(lakeassessed_2022)[3] <- "ASSESSMENT"
riverassessed_2022 <- riverassessed_2022[c("ASSESSMENT", "CT2022_REC_USE_ATTAINMENT")]
lakeassessed_2022 <- lakeassessed_2022[c("ASSESSMENT", "CT2022_REC_USE_ATTAINMENT")]
assessed_2022 <- rbind(riverassessed_2022, lakeassessed_2022)
segment_analysis<- left_join(segment_analysis, assessed_2022, by = "ASSESSMENT") #I want to keep NA segment analysis values
View(segment_analysis)
write.csv(segment_analysis, "segment_analysis.csv", row.names = FALSE) #to make final assessment decisions
View(segment_analysis)
View(segment_analysis)
x <- 2 #this creates a variable 'x' with the value 2
y <- 3 #this creates a variable 'y' with the value 3
x + y #this will print the evaluated expression in the console
z <- x + y #this assigns the result to a new variable, z
z #running just z will output the same result as x + y (5)
#we can also create characters and lists that may be useful for manipulating data
sitenames <- c("Salmon River", "Roaring Brook", "Hedge Brook")
sitenames #this prints the list of names that we just created
#most commonly, we will be reading in data files that already exist
#it is INCREDIBLY IMPORTANT that you make copies of any files you are going to
#manipulate in R - place the copy in your working directory is. do not simply
#move a file off a drive into your own folder!
#lets make some data frames! often abbreviated to 'df' in online guides
hoboData <- read.csv("hoboData.csv") #this makes a dataframe called 'hoboData'
#instructions: create an "R" folder on your P drive. then, copy the 'seasonalRintro'
#file you have been given to the R folder. the entire address will be P:\R\seasonalRintro
setwd("P:/R/seasonalRintro") #this sets the working directory to the folder we just made
#lets make some data frames! often abbreviated to 'df' in online guides
hoboData <- read.csv("hoboData.csv") #this makes a dataframe called 'hoboData'
#click on the hoboData object in the 'Data' view to explore the columns
#columns can be filtered and sorted to quickly visualize their contents
hoboData #this prints the dataframe in the console
hoboData$ProbeID #this is how we refer to individual columns in a data frame
hoboData$NewCol <- NA #this is how we make new columns, this is populated with NA values
sapply(hoboData, class) #this prints the data types of our data frame in the console
#data types are important because we cannot manipulate data if we are assuming
#the incorrect data type. for example, we can't do math on a character or work
#with dates correctly unless we fix data types as needed. this is a critical first
#thing to check if you are finding that you cannot summarize or visualize data
#correctly!
#if we try to plot the data with base R, we will get an error
plot(hoboData$Date_Time, hoboData$Temp)
hoboDataFix <- hoboData #without changes, the Fix version is identical to the original
hoboDataFix$Temp <- as.numeric(hoboDataFix$Temp) #this forces the data type of the Temp
#column to numeric. this will change any cells with character data to a NA null value
#which is an expected result and OK for what we are doing
hoboDataFix$Date_Time <- as.POSIXct(hoboDataFix$Date_Time, format = "%m/%d/%Y %H:%M")
View(hoboDataFix)
hoboDataFix <- hoboData #without changes, the Fix version is identical to the original
hoboDataFix$Temp <- as.numeric(hoboDataFix$Temp) #this forces the data type of the Temp
#column to numeric. this will change any cells with character data to a NA null value
#which is an expected result and OK for what we are doing
hoboDataFix$Date_Time <- as.POSIXct(hoboDataFix$Date_Time, format = "%m/%d/%Y %H:%M")
#this converts the character Date_Time column to a POSIXct format, which stores date
#and time information. this means R will now "know" how to correctly handle this data
#as a datetime and not as a character. you may have datetimes that come in with
#very different formats. the 'format = ' argument is mapped to what the original data
#looks like. for example, another common format would be 2022-09-30 or %Y-%m-%d
#lets see if our fixes worked!
plot(hoboDataFix$Date_Time, hoboDataFix$Temp) #syntax: plot(x variable, y variable)
#you have now successfully made a plot of corrected data!
#vertically merging:
hoboDataBind <- read.csv("hoboDataBind.csv") #temperature data from a different site
colnames(hoboDataBind) #checking the column names
colnames(hoboData) #checking the column names
#the column names are the same as hoboData, so we can use rbind to vertically
#merge these two files
hoboDataVBound <- rbind(hoboDataBind, hoboData) #if we count the number of rows
View(hoboDataBind)
View(hoboData)
View(hoboDataFix)
colnames(hoboData) #print the new column names
colnames(hoboData) #print the new column names
hoboData <- hoboData[c("Date_Time","Temp","UOM","ProbeID","SID" ,"Collector","ProbeType")] #removing NewCol
sapply(hoboData, class) #this prints the data types of our data frame in the console
#data types are important because we cannot manipulate data if we are assuming
#the incorrect data type. for example, we can't do math on a character or work
#with dates correctly unless we fix data types as needed. this is a critical first
#thing to check if you are finding that you cannot summarize or visualize data
#correctly!
#we have discovered that Date_Time and Temp are both character classes, but to
#manipulate them, they need to be a datetime and a numeric class, respectively
#sometimes we mess up when fixing data types, so lets create a new version of
#hoboData to compare to the original
hoboDataFix <- hoboData #without changes, the Fix version is identical to the original
hoboDataFix$Temp <- as.numeric(hoboDataFix$Temp) #this forces the data type of the Temp
#column to numeric. this will change any cells with character data to a NA null value
#which is an expected result and OK for what we are doing
hoboDataFix$Date_Time <- as.POSIXct(hoboDataFix$Date_Time, format = "%m/%d/%Y %H:%M")
#this converts the character Date_Time column to a POSIXct format, which stores date
#and time information. this means R will now "know" how to correctly handle this data
#as a datetime and not as a character. you may have datetimes that come in with
#very different formats. the 'format = ' argument is mapped to what the original data
#looks like. for example, another common format would be 2022-09-30 or %Y-%m-%d
#lets see if our fixes worked!
plot(hoboDataFix$Date_Time, hoboDataFix$Temp) #syntax: plot(x variable, y variable)
#you have now successfully made a plot of corrected data!
#vertically merging:
hoboDataBind <- read.csv("hoboDataBind.csv") #temperature data from a different site
colnames(hoboDataBind) #checking the column names
colnames(hoboData) #checking the column names
#the column names are the same as hoboData, so we can use rbind to vertically
#merge these two files
hoboDataVBound <- rbind(hoboDataBind, hoboData) #if we count the number of rows
#in hoboDataVBound, we can see that they equal the sum of the two files
#horizontally merging:
stations <- read.csv("stations.csv") #an output from our AWX stations table
colnames(stations) #checking the column names
#common monitoring terminology: "STA_SEQ" "AWX Station" "SID" all refer to the
#unique site ID that is stored in the AWX database. stations$STA_SEQ and
#hoboDATA$SID refer to the same thing. we can horizontally merge by this shared
#parameter, but we first need to change the names of columns so that they match
#R needs to have a shared column name with shared values to be able to merge in this way
#here is how we can change the name of a single named column in a data frame
names(stations)[names(stations) == "STA_SEQ"] <- "SID" #the AWX column in hoboData
colnames(stations) #print to see changed names
hoboDataHBound <- merge(hoboData, stations, by = "SID")
sapply(hoboDataVBound, class) #Date_Time and Temp are characters and we need to fix that
hoboDataVBound$Temp <- as.numeric(hoboDataVBound$Temp) #same code as before
hoboDataVBound$Date_Time <- as.POSIXct(hoboDataVBound$Date_Time, format = "%m/%d/%Y %H:%M") #same code as before
hoboDataVBound$Date <- as.Date(hoboDataVBound$Date_Time) #this makes a new column with Date from Date_Time
View(hoboDataVBound)
#dplyr portion! we will add a new kind of syntax, the pipe operator: %>%. this operator
#can be read as "and then." e.g. "make a new data frame and then group by (x) and then
#calculate the mean of (x)... etc
hoboDataSums <- hoboDataVBound %>% #pipe operator
group_by(SID, Date) %>% #first group on site ID, then on the Date as we want 24hr averages
summarize(Mean_24hr = mean(Temp)) #this creates a new column called 'Mean24hr' with the mean of Temp
View(hoboDataSums)
#this new data frame contains 24hr Temp averages for each SID (site ID) and date
#we can quickly plot and visualize it
plot(hoboDataSums$Date, hoboDataSums$Mean_24hr) #one issue with this graph is that
#it does not differentiate between the two sites, even though we differentiated
#in our group_by line. R dataframes can also be subset according to logical rules
#lets try subsetting hoboDataSums by one of the site IDs
hoboDataSums_subset <- subset(hoboDataSums, hoboDataSums$SID == "15499")# syntax:
#subset hobodata sums where the SID column equals (==, not =) 15499. other logical
#operators include & (and), | (or), >, <, >=, <=, etc. we could also try subsetting
#for only certain values. for example:
hoboDataSums_subset <- subset(hoboDataSums, hoboDataSums$Mean_24hr >= 17) #now we
#only return rows for which the 24hr temperature is greater than or equal to 17 deg C
#checking for NAs can also be helpful and the syntax is different
hoboDataSumsNA <- subset(hoboDataSums, is.na(hoboDataSums$Mean_24hr)) #this returns
#rows where the 24hr average was NA (this is due to having NAs in the original data)
#to have dplyr ignore NAs when summarizing, you can add modify the summarize line
#to look like summarize(Mean_24hr = mean(Temp, na.rm = TRUE)). this makes it calculate
#means even if the grouping has NAs
#another thing you may want to do is add calculated columns. the syntax is
#fairly straightforward
hoboDataSums$Mult <- hoboDataSums$Mean_24hr * 100 #making a new column that is
#multiplying the mean column by 100. like Excel functions, you can also add, subtract
#divide, etc. use PEMDAS!
hoboData <- read.csv("hoboData.csv") #read in some example data
hoboData[1:10,]
